{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Book Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/books_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "its only art if its well hung julie strain \n",
      "                                         ID     Score\n",
      "209127                   Hung by the tongue  0.365535\n",
      "203498                  Julie Of The Wolves  0.321138\n",
      "65990                            The Return  0.290065\n",
      "40737                The RETURN: THE RETURN  0.288097\n",
      "167024  Gardening Without Stress and Strain  0.282020\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "df[\"Clean Title\"] = df['Title'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "df['combined_features'] = df['Clean Title'].fillna('') + ' ' + df['authors'].fillna('') + ' ' + df['description'].fillna('')\n",
    "df['combined_features'] = df['combined_features'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "# Remove any non-unique rows\n",
    "df = df.drop_duplicates(subset=['Clean Title'])\n",
    "\n",
    "# Define variables\n",
    "ID_COLUMN = 'Title'\n",
    "COMPARISON_COLUMN = 'combined_features'\n",
    "SPECIFIC_ID = 'Its Only Art If Its Well Hung!'\n",
    "\n",
    "# Vectorize the comments\n",
    "v = TfidfVectorizer(stop_words='english')\n",
    "X = v.fit_transform(df[COMPARISON_COLUMN])\n",
    "\n",
    "# Map IDs to indices\n",
    "Id2idx = pd.Series(df.index, index=df[ID_COLUMN])\n",
    "\n",
    "def get_most_similar(id):\n",
    "    idx = Id2idx[id]\n",
    "    scores = cosine_similarity(X, X[idx]).flatten()\n",
    "    recommended = (-scores).argsort()[1:6]\n",
    "    return df[ID_COLUMN].iloc[recommended], scores[recommended]\n",
    "\n",
    "# Get similar items\n",
    "similar = get_most_similar(SPECIFIC_ID)\n",
    "\n",
    "# Create DataFrame with results\n",
    "df_similar = pd.DataFrame({\n",
    "    'ID': similar[0],\n",
    "    'Score': similar[1],\n",
    "    'Comment': similar[0].apply(lambda x: df[df[ID_COLUMN] == x][COMPARISON_COLUMN].values[0])\n",
    "})\n",
    "\n",
    "print('Original:')\n",
    "print(df[df[ID_COLUMN] == SPECIFIC_ID][COMPARISON_COLUMN].values[0])\n",
    "\n",
    "print(df_similar[[\"ID\", \"Score\"]].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "\n",
    "For now, let's keep model evaluation simple by sampling a couple recommendations and manually reviewing. Later, we'll take a more complex approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book: The Sierra Club: Mountain Light Postcard Collection: A Portfolio\n",
      "Top rec: The Encyclopedia of Ancient Civilizations\n",
      "Book: Starting and Succeeding in Real Estate\n",
      "Top rec: The Official XMLSPY Handbook\n",
      "Book: The Eye of the Abyss (Franz Schmidt, 1)\n",
      "Top rec: The Art of Translating Prose\n",
      "Book: The Kabbalah Pillars: A Romance of The Ages\n",
      "Top rec: How To Make The Devil Obey You!!!\n",
      "Book: Iridescent Soul\n",
      "Top rec: Wallace Stevens: A Poet's Growth\n"
     ]
    }
   ],
   "source": [
    "# Manual review\n",
    "\n",
    "# Sample five books to assess recommendations\n",
    "sample_books = df.sample(5, random_state=20)\n",
    "\n",
    "top_recs = []\n",
    "for _, row in sample_books.iterrows():\n",
    "    book_recs = get_most_similar(row[\"Title\"])\n",
    "    top_recs.append(book_recs[0])\n",
    "\n",
    "# Print the sampled titles with top recommendations for that title\n",
    "for i, rec in enumerate(top_recs):\n",
    "    print(f\"Book: {sample_books.iloc[i]['Title']}\")\n",
    "    print(f\"Top rec: {rec.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our recommendations seem a bit all over the place right now. In part 3, we'll do some work to improve the quality of our recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Composite Book Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ratings = pd.read_csv('../data/Books_rating.csv')\n",
    "ratings[\"Clean Title\"] = ratings[\"Title\"].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/adene/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from scipy.sparse import hstack\n",
    "import numpy as np\n",
    "\n",
    "ratings_sample = ratings.sample(frac = 0.05, random_state = 42)\n",
    "\n",
    "# TODO: Build review/text and review/summary -> review/score model\n",
    "v = TfidfVectorizer(stop_words='english')\n",
    "combined = ratings_sample[\"review/summary\"].fillna(\"\") + \" \" + ratings_sample[\"review/text\"].fillna(\"\")\n",
    "\n",
    "\n",
    "# Add Vader sentiment analysis\n",
    "sid_obj = SentimentIntensityAnalyzer()\n",
    "ratings_sample[\"Polarity\"] = ratings_sample[\"review/summary\"].fillna(\"\").apply(lambda x: sid_obj.polarity_scores(x)[\"compound\"])\n",
    "\n",
    "# Convert polarity series to a 2D numpy array (shape: n_samples x 1)\n",
    "X_polarity = np.array(ratings_sample[\"Polarity\"]).reshape(-1, 1)\n",
    "\n",
    "# Get the TF-IDF features (sparse matrix)\n",
    "X_text = v.fit_transform(combined)\n",
    "\n",
    "# Combine the two using hstack so that each row is a concatenation of TF-IDF features and the polarity value.\n",
    "X = hstack([X_text, X_polarity])\n",
    "y = ratings_sample[\"review/score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: MAE= 0.766 , MSE= 1.099 , RMSE= 1.049 , R-squared:  0.236\n",
      "Training: MAE= 0.708 , MSE= 0.929 , RMSE= 0.964 , R-squared:  0.358\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "model = tree.DecisionTreeRegressor(max_depth=15)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "test_predictions = model.predict(X_test)\n",
    "train_predictions = model.predict(X_train)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "DECIMALS = 3\n",
    "\n",
    "# Testing metrics\n",
    "test_MAE = mean_absolute_error(y_test, test_predictions)\n",
    "test_MSE = mean_squared_error(y_test, test_predictions)\n",
    "test_RMSE = np.sqrt(test_MSE)\n",
    "test_r2 = r2_score(y_test, test_predictions)\n",
    "print('Testing: MAE=', round(test_MAE, DECIMALS), ', MSE=', round(test_MSE, DECIMALS), ', RMSE=', round(test_RMSE, DECIMALS), ', R-squared: ', round(test_r2, DECIMALS))\n",
    "\n",
    "# Training metrics\n",
    "train_MAE = mean_absolute_error(y_train, train_predictions)\n",
    "train_MSE = mean_squared_error(y_train, train_predictions)\n",
    "train_RMSE = np.sqrt(train_MSE)\n",
    "train_r2 = r2_score(y_train, train_predictions)\n",
    "print('Training: MAE=', round(train_MAE, DECIMALS), ', MSE=', round(train_MSE, DECIMALS), ', RMSE=', round(train_RMSE, DECIMALS), ', R-squared: ', round(train_r2, DECIMALS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10355/10355 [00:22<00:00, 464.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# To predict the average review, let's predict individual reviews and average\n",
    "\n",
    "# Group by book from ratings, sample 5% of total books, \n",
    "unique_titles = ratings[\"Clean Title\"].drop_duplicates()\n",
    "\n",
    "sample_books = unique_titles.sample(frac = 0.05, random_state = 42)\n",
    "\n",
    "grouped = ratings.groupby(\"Clean Title\")\n",
    "\n",
    "abs_error = []\n",
    "\n",
    "for title in tqdm(sample_books):\n",
    "    # Retrieve all rows (i.e., reviews) for this book.\n",
    "    book_reviews = grouped.get_group(title).copy()\n",
    "\n",
    "    book_reviews.loc[:, \"Polarity\"] = book_reviews[\"review/summary\"].fillna(\"\").apply(lambda x: sid_obj.polarity_scores(x)[\"compound\"])\n",
    "\n",
    "    # Convert polarity series to a 2D numpy array (shape: n_samples x 1)\n",
    "    X_polarity = np.array(book_reviews[\"Polarity\"]).reshape(-1, 1)\n",
    "\n",
    "    # Get the TF-IDF features (sparse matrix)\n",
    "    X_text = v.transform(book_reviews[\"review/summary\"].fillna(\"\") + \" \" + book_reviews[\"review/text\"].fillna(\"\"))\n",
    "\n",
    "    # Combine the two using hstack so that each row is a concatenation of TF-IDF features and the polarity value.\n",
    "    X = hstack([X_text, X_polarity])\n",
    "    y = ratings_sample[\"review/score\"]\n",
    "    \n",
    "    predicted_scores = model.predict(X)\n",
    "    avg_pred = np.mean(predicted_scores)\n",
    "    \n",
    "    # Here, we compute the actual average review score:\n",
    "    avg_score = book_reviews[\"review/score\"].mean()\n",
    "    \n",
    "    abs_error.append(abs(avg_pred - avg_score))\n",
    "\n",
    "print(\"Mean absolute error:\", round(np.mean(abs_error), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By averaging individual predictions we achieved a MAE of 0.52. This is a 0.34 improvement from the null model of predicting the average rating for all groups of books. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation of average ratings per book: 0.824\n"
     ]
    }
   ],
   "source": [
    "# Find the grouped standard deviation of average ratings per book\n",
    "averages = grouped[\"review/score\"].mean()\n",
    "std_dev = averages.std()\n",
    "print(\"Standard deviation of average ratings per book:\", round(std_dev, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Comprehensive Recommender System\n",
    "\n",
    "To improve upon our recommendation model from part 1, let's consider which books users tend to like if they also liked the reference book. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potentially much simpler solution\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/books_data.csv')\n",
    "ratings = pd.read_csv('../data/Books_rating.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>User_id</th>\n",
       "      <th>profileName</th>\n",
       "      <th>review/helpfulness</th>\n",
       "      <th>review/score</th>\n",
       "      <th>review/time</th>\n",
       "      <th>review/summary</th>\n",
       "      <th>review/text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1882931173</td>\n",
       "      <td>Its Only Art If Its Well Hung!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AVCGYZL8FQQTD</td>\n",
       "      <td>Jim of Oz \"jim-of-oz\"</td>\n",
       "      <td>7/7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>940636800</td>\n",
       "      <td>Nice collection of Julie Strain images</td>\n",
       "      <td>This is only for Julie Strain fans. It's a col...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0826414346</td>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A30TK6U7DNS82R</td>\n",
       "      <td>Kevin Killian</td>\n",
       "      <td>10/10</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1095724800</td>\n",
       "      <td>Really Enjoyed It</td>\n",
       "      <td>I don't care much for Dr. Seuss but after read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0826414346</td>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A3UH4UZ4RSVO82</td>\n",
       "      <td>John Granger</td>\n",
       "      <td>10/11</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1078790400</td>\n",
       "      <td>Essential for every personal and Public Library</td>\n",
       "      <td>If people become the books they read and if \"t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0826414346</td>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A2MVUWT453QH61</td>\n",
       "      <td>Roy E. Perry \"amateur philosopher\"</td>\n",
       "      <td>7/7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1090713600</td>\n",
       "      <td>Phlip Nel gives silly Seuss a serious treatment</td>\n",
       "      <td>Theodore Seuss Geisel (1904-1991), aka &amp;quot;D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0826414346</td>\n",
       "      <td>Dr. Seuss: American Icon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A22X4XUPKF66MR</td>\n",
       "      <td>D. H. Richards \"ninthwavestore\"</td>\n",
       "      <td>3/3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1107993600</td>\n",
       "      <td>Good academic overview</td>\n",
       "      <td>Philip Nel - Dr. Seuss: American IconThis is b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Id                           Title  Price         User_id  \\\n",
       "0  1882931173  Its Only Art If Its Well Hung!    NaN   AVCGYZL8FQQTD   \n",
       "1  0826414346        Dr. Seuss: American Icon    NaN  A30TK6U7DNS82R   \n",
       "2  0826414346        Dr. Seuss: American Icon    NaN  A3UH4UZ4RSVO82   \n",
       "3  0826414346        Dr. Seuss: American Icon    NaN  A2MVUWT453QH61   \n",
       "4  0826414346        Dr. Seuss: American Icon    NaN  A22X4XUPKF66MR   \n",
       "\n",
       "                          profileName review/helpfulness  review/score  \\\n",
       "0               Jim of Oz \"jim-of-oz\"                7/7           4.0   \n",
       "1                       Kevin Killian              10/10           5.0   \n",
       "2                        John Granger              10/11           5.0   \n",
       "3  Roy E. Perry \"amateur philosopher\"                7/7           4.0   \n",
       "4     D. H. Richards \"ninthwavestore\"                3/3           4.0   \n",
       "\n",
       "   review/time                                   review/summary  \\\n",
       "0    940636800           Nice collection of Julie Strain images   \n",
       "1   1095724800                                Really Enjoyed It   \n",
       "2   1078790400  Essential for every personal and Public Library   \n",
       "3   1090713600  Phlip Nel gives silly Seuss a serious treatment   \n",
       "4   1107993600                           Good academic overview   \n",
       "\n",
       "                                         review/text  \n",
       "0  This is only for Julie Strain fans. It's a col...  \n",
       "1  I don't care much for Dr. Seuss but after read...  \n",
       "2  If people become the books they read and if \"t...  \n",
       "3  Theodore Seuss Geisel (1904-1991), aka &quot;D...  \n",
       "4  Philip Nel - Dr. Seuss: American IconThis is b...  "
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing...\n",
      "Setting up book vectors...\n"
     ]
    }
   ],
   "source": [
    "# Pairwise recommendation tuning\n",
    "# Filter & sample reference books (rating 5, 5% sample)\n",
    "ratings_renamed = ratings.rename(columns={\"User_id\": \"user\", \"review/score\": \"rating\"})\n",
    "df['combined_features'] = df['Title'].fillna('') + ' ' + df['authors'].fillna('') + ' ' + df['description'].fillna('')\n",
    "df['combined_features'] = df['combined_features'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "df[\"Clean Title\"] = df['Title'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "ratings_renamed[\"Clean Title\"] = ratings_renamed['Title'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "df_merge = pd.merge(df, ratings_renamed[[\"Clean Title\", \"user\", \"rating\"]], on=\"Clean Title\", how=\"left\")\n",
    "\n",
    "print(\"Vectorizing...\")\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(df['combined_features'])\n",
    "print(\"Setting up book vectors...\")\n",
    "book_vectors = {i: tfidf_matrix[i] for i in range(tfidf_matrix.shape[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19805"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a dictionary mapping each Clean Title (converted to string, lowercased, and stripped) to its first index in df.\n",
    "clean_title_to_index = {}\n",
    "for idx, title in df[\"Clean Title\"].astype(str).items():\n",
    "    cleaned_title = title.strip().lower()\n",
    "    if cleaned_title not in clean_title_to_index:\n",
    "        clean_title_to_index[cleaned_title] = idx\n",
    "\n",
    "clean_title_to_index['the great gatsby']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the TF-IDF matrix (X) from the combined features column\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['combined_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top5_recommendations(ref_idx, rating_threshold=5, top_n=5):\n",
    "    # Use df for reference lookup so indices match X and book_vectors\n",
    "    ref_title = df.loc[ref_idx, \"Clean Title\"]\n",
    "    ref_vector = X[ref_idx]  # Using the TF-IDF matrix from df\n",
    "     \n",
    "    # Find users who rated the reference book with the desired threshold (fallback to threshold-1)\n",
    "    users = set(df_merge[(df_merge[\"Clean Title\"] == ref_title) & (df_merge[\"rating\"] == rating_threshold)][\"user\"])\n",
    "    if not users:\n",
    "        users = set(df_merge[(df_merge[\"Clean Title\"] == ref_title) & (df_merge[\"rating\"] == rating_threshold - 1)][\"user\"])\n",
    "    \n",
    "    candidates = pd.DataFrame()\n",
    "    if users:\n",
    "        # Filter candidate reviews from these users (and exclude the ref book)\n",
    "        candidates = df_merge[\n",
    "            (df_merge[\"user\"].isin(users)) &\n",
    "            (df_merge[\"rating\"] == rating_threshold) &\n",
    "            (df_merge[\"Clean Title\"] != ref_title)\n",
    "        ].drop_duplicates(subset=[\"Clean Title\"])\n",
    "    if len(candidates) == 0:\n",
    "        # If no such users, default: use all books except the reference, taking only unique titles\n",
    "        candidates = df_merge[df_merge[\"Clean Title\"] != ref_title].drop_duplicates(subset=[\"Clean Title\"])\n",
    "        \n",
    "    # Build a list of candidate indices corresponding to df.\n",
    "    # (Remember: book_vectors and X were built on df, not df_merge.)\n",
    "    candidate_df_indices = []  # Indices from df\n",
    "    candidate_mapping = {}     # Map from candidate index in df to candidate index in df_merge\n",
    "    for merge_idx in candidates.index:\n",
    "        candidate_title = df_merge.loc[merge_idx, \"Clean Title\"]\n",
    "        if pd.isna(candidate_title):\n",
    "            continue\n",
    "        candidate_key = candidate_title.strip().lower()\n",
    "        if candidate_key in clean_title_to_index:\n",
    "            df_idx = clean_title_to_index[candidate_key]\n",
    "            candidate_df_indices.append(df_idx)\n",
    "            candidate_mapping[df_idx] = merge_idx  # record mapping back to df_merge\n",
    "    \n",
    "    if not candidate_df_indices:\n",
    "        return pd.DataFrame()  # No candidates found\n",
    "    \n",
    "    # Compute cosine similarities in one batch:\n",
    "    candidate_matrix = X[candidate_df_indices]\n",
    "    sim_values = cosine_similarity(ref_vector, candidate_matrix)[0]\n",
    "    \n",
    "    # Create a list of tuples: (merge_idx, similarity)\n",
    "    candidate_sim_tuples = []\n",
    "    for df_idx, sim in zip(candidate_df_indices, sim_values):\n",
    "        merge_idx = candidate_mapping[df_idx]\n",
    "        candidate_sim_tuples.append((merge_idx, sim))\n",
    "    \n",
    "    # Sort by similarity (highest first) and select top_n candidates\n",
    "    top_candidates = sorted(candidate_sim_tuples, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    \n",
    "    recs = []\n",
    "    for merge_idx, sim in top_candidates:\n",
    "        candidate_title = df_merge.loc[merge_idx, \"Clean Title\"]\n",
    "        # Get the corresponding df index from the lookup dictionary\n",
    "        df_candidate_idx = clean_title_to_index[candidate_title.strip().lower()]\n",
    "        recs.append({\n",
    "            \"candidate_index\": df_candidate_idx,\n",
    "            \"Title\": df.loc[df_candidate_idx, \"Title\"],\n",
    "            \"similarity\": sim,\n",
    "            \"rating\": df_merge.loc[merge_idx, \"rating\"]\n",
    "        })\n",
    "    return pd.DataFrame(recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference book: Harry Potter and the Chamber of Secrets\n",
      "   candidate_index                                              Title  \\\n",
      "0            59516  Harry Potter Y La Camara Secreta (Harry Potter...   \n",
      "1           124209  Harry Potter And The Chamber Of Secrets/ Teach...   \n",
      "2           136930  There's Something About Harry: A Catholic Anal...   \n",
      "3            16478         Harry Potter und der Gefangene von Azkaban   \n",
      "4            36776              Harry Potter and The Sorcerer's Stone   \n",
      "\n",
      "   similarity  rating  \n",
      "0    0.434626     5.0  \n",
      "1    0.428419     5.0  \n",
      "2    0.413875     5.0  \n",
      "3    0.409391     5.0  \n",
      "4    0.399634     5.0  \n"
     ]
    }
   ],
   "source": [
    "ref_idx = clean_title_to_index['harry potter and the chamber of secrets']\n",
    "# ref_idx = clean_title_to_index['the great gatsby']\n",
    "# ref_idx = 101\n",
    "top5_recs = get_top5_recommendations(ref_idx)\n",
    "print(\"Reference book:\", df.loc[ref_idx, \"Title\"])\n",
    "print(top5_recs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
